{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow import SparseTensor, sparse, function, random, squeeze, saved_model, constant, losses, strings\n",
    "from tensorflow import data as dt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"drag-names.txt\", 'r').read()\n",
    "text = unicodedata.normalize('NFKD',file)\n",
    "vocab = sorted(set(text))\n",
    "ids_from_chars = StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "chars_from_ids = StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids_from_chars(strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = dt.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(dt.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "    self.dense = Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 3.6535\n",
      "Epoch 2/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 3.4617\n",
      "Epoch 3/125\n",
      "7/7 [==============================] - 11s 2s/step - loss: 3.2817\n",
      "Epoch 4/125\n",
      "7/7 [==============================] - 11s 2s/step - loss: 3.0281\n",
      "Epoch 5/125\n",
      "7/7 [==============================] - 11s 2s/step - loss: 2.9220\n",
      "Epoch 6/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.8290\n",
      "Epoch 7/125\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.7328\n",
      "Epoch 8/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.6548\n",
      "Epoch 9/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.5966\n",
      "Epoch 10/125\n",
      "7/7 [==============================] - 11s 2s/step - loss: 2.5576\n",
      "Epoch 11/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.5269\n",
      "Epoch 12/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.5040\n",
      "Epoch 13/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.4824\n",
      "Epoch 14/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.4648\n",
      "Epoch 15/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.4427\n",
      "Epoch 16/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.4203\n",
      "Epoch 17/125\n",
      "7/7 [==============================] - 11s 2s/step - loss: 2.4034\n",
      "Epoch 18/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.3874\n",
      "Epoch 19/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.3742\n",
      "Epoch 20/125\n",
      "7/7 [==============================] - 9s 1s/step - loss: 2.3614\n",
      "Epoch 21/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.3517\n",
      "Epoch 22/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 2.3382\n",
      "Epoch 23/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.3271\n",
      "Epoch 24/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.3150\n",
      "Epoch 25/125\n",
      "7/7 [==============================] - 11s 2s/step - loss: 2.3073\n",
      "Epoch 26/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.2931\n",
      "Epoch 27/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.2809\n",
      "Epoch 28/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.2678\n",
      "Epoch 29/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.2536\n",
      "Epoch 30/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.2411\n",
      "Epoch 31/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.2275\n",
      "Epoch 32/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.2118\n",
      "Epoch 33/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.1965\n",
      "Epoch 34/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.1799\n",
      "Epoch 35/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 2.1628\n",
      "Epoch 36/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.1370\n",
      "Epoch 37/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.1210\n",
      "Epoch 38/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.0994\n",
      "Epoch 39/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.0763\n",
      "Epoch 40/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.0515\n",
      "Epoch 41/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 2.0240\n",
      "Epoch 42/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 1.9924\n",
      "Epoch 43/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 1.9612\n",
      "Epoch 44/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 1.9285\n",
      "Epoch 45/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 1.8942\n",
      "Epoch 46/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 1.8573\n",
      "Epoch 47/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 1.8147\n",
      "Epoch 48/125\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.7726\n",
      "Epoch 49/125\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.7257\n",
      "Epoch 50/125\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.6726\n",
      "Epoch 51/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 1.6222\n",
      "Epoch 52/125\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.5670\n",
      "Epoch 53/125\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.5099\n",
      "Epoch 54/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 1.4546\n",
      "Epoch 55/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 1.3884\n",
      "Epoch 56/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 1.3209\n",
      "Epoch 57/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 1.2514\n",
      "Epoch 58/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 1.1809\n",
      "Epoch 59/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 1.1066\n",
      "Epoch 60/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 1.0296\n",
      "Epoch 61/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.9547\n",
      "Epoch 62/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.8781\n",
      "Epoch 63/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.7987\n",
      "Epoch 64/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.7247\n",
      "Epoch 65/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.6513\n",
      "Epoch 66/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.5820\n",
      "Epoch 67/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.5123\n",
      "Epoch 68/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.4524\n",
      "Epoch 69/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.3996\n",
      "Epoch 70/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.3514\n",
      "Epoch 71/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.3090\n",
      "Epoch 72/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.2710\n",
      "Epoch 73/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.2369\n",
      "Epoch 74/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.2097\n",
      "Epoch 75/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.1870\n",
      "Epoch 76/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.1679\n",
      "Epoch 77/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.1512\n",
      "Epoch 78/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.1383\n",
      "Epoch 79/125\n",
      "7/7 [==============================] - 17s 2s/step - loss: 0.1278\n",
      "Epoch 80/125\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.1187\n",
      "Epoch 81/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.1115\n",
      "Epoch 82/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.1052\n",
      "Epoch 83/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.1000\n",
      "Epoch 84/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0957\n",
      "Epoch 85/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0920\n",
      "Epoch 86/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0882\n",
      "Epoch 87/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0853\n",
      "Epoch 88/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0826\n",
      "Epoch 89/125\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0800\n",
      "Epoch 90/125\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.0779\n",
      "Epoch 91/125\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0758\n",
      "Epoch 92/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0739\n",
      "Epoch 93/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0722\n",
      "Epoch 94/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0705\n",
      "Epoch 95/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0691\n",
      "Epoch 96/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0675\n",
      "Epoch 97/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0663\n",
      "Epoch 98/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0652\n",
      "Epoch 99/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0640\n",
      "Epoch 100/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0629\n",
      "Epoch 101/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0618\n",
      "Epoch 102/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0607\n",
      "Epoch 103/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0596\n",
      "Epoch 104/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0590\n",
      "Epoch 105/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0582\n",
      "Epoch 106/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0573\n",
      "Epoch 107/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0564\n",
      "Epoch 108/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0557\n",
      "Epoch 109/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0550\n",
      "Epoch 110/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0543\n",
      "Epoch 111/125\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0537\n",
      "Epoch 112/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0531\n",
      "Epoch 113/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0526\n",
      "Epoch 114/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0518\n",
      "Epoch 115/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0514\n",
      "Epoch 116/125\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0509\n",
      "Epoch 117/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0502\n",
      "Epoch 118/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0498\n",
      "Epoch 119/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0492\n",
      "Epoch 120/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0488\n",
      "Epoch 121/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0485\n",
      "Epoch 122/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0481\n",
      "Epoch 123/125\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0474\n",
      "Epoch 124/125\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0472\n",
      "Epoch 125/125\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.0468\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = \"weights1/DragNames1.h5\"\n",
    "one_step_model.save_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fe8208b5460>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.preprocessing.string_lookup.StringLookup object at 0x7fe81fe43460>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as generate_one_step, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_drag_name():\n",
    "  import random\n",
    "  states = None\n",
    "  first_choice = ['a','e','i','n','r','l','o','s','t','m','d','c','h','y','u','k','b','v','g','p','x','f','j','w','z','q']\n",
    "  second_choice = {'a': ['aa','ab','ac','ad','af','ag','ai','aj','ak','al','am','an','ap','aq','ar','as','at','au','av','aw','ax','ay','az'],'b': ['b ','ba','be','bi','bl','bo','br','bu'],'c': ['ca','ce','ch','ci','cl','co','cr','ct','cu','cy'],'d': ['da','dd','de','di','dm','do','dr','du','dw','dy','dé','dí'],'e': ['eb','ec','ed','eg','el','em','en','ep','er','es','et','eu','ev','ex'],'f': ['fa','fe','fi','fk','fl','fo','fr'],'g': ['ga','gd','ge','gi','gl','go','gr','gu','gy'],'h': ['ha','he','hi','ho','hu'],'i': ['ic','id','ig','il','im','in','io','ir','is','iv','iy','iz'],'j': ['ja','jd','je','jf','ji','jo','ju','jy'],'k': ['ka','kc','ke','kh','ki','kl','ko','kr','ky'],'l': ['la','lc','le','li','lo','lq','lu','ly'],'m': ['m ','ma','me','mh','mi','mo','mr','ms','mu','mx','my','mz'],'n': ['na','ne','ni','no','nu','ny'],'o': ['ob','oc','ol','om','on','op','or','ot','ox'],'p': ['pa','pe','ph','pi','pl','pm','po','pr','ps','pu','py','pé'],'q': ['qu','qy'],'r': ['ra','re','rh','ri','ro','ru','ry'],'s': ['sa','sc','se','sh','si','sk','sl','sm','so','sp','sr','st','su','sv','sy'],'t': ['t ','ta','te','th','ti','to','tp','tr','ts','tu','tw','ty','tí','tó'],'u': ['uc','uh','ul','um','un','ur','ut'],'v': ['va','ve','vi','vo'],'w': ['wa','we','wh','wi','wo','wy'],'x': ['xa','xe','xi','xo','xt','xu'],'y': ['ya','ye','yo','yu','yv'],'z': ['za','ze','zi','zo','zs','zy']}\n",
    "  choose1 = random.choice(first_choice)\n",
    "  choose2 = random.choice(second_choice[choose1])\n",
    "\n",
    "  next_char = constant([choose2])\n",
    "  result = [next_char]\n",
    "\n",
    "  for n in range(32):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    if next_char == '\\n':\n",
    "      break\n",
    "    result.append(next_char)\n",
    "\n",
    "  result = strings.join(result)\n",
    "  names = result[0].numpy().decode('utf-8').split(' ')\n",
    "  capital = [name.capitalize() for name in names]\n",
    "  return ' '.join(capital).replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind\n",
      "Suse\n",
      "Zy Bisch\n",
      "Janessa Highland\n",
      "Nerva\n",
      "Toute Havet\n",
      "M Butalee\n",
      "Queet\n",
      "Xton\n",
      "Ickari\n",
      "Gloo\n",
      "Ision\n",
      "Yous Love\n",
      "Yutuve\n",
      "Ettie Rebel\n",
      "Du Chey\n",
      "Yonca\n",
      "Fkras Dolai\n",
      "Want\n",
      "Icka\n",
      "Treale Knight\n",
      "Mhona\n",
      "Ts\n",
      "Dia Elektra\n",
      "Kress Poxxi\n",
      "Wind\n",
      "Litalityssa Hillz\n",
      "Caso\n",
      "Lowle Mcraghor\n",
      "Jd\n",
      "Rubie\n",
      "Von Lee\n",
      "Kley\n",
      "Le Hepen\n",
      "Qyuina\n",
      "Zon\n",
      "Poria Nithose\n",
      "Phine\n",
      "Olly Maid\n",
      "Undie Sw James\n",
      "Nusmi\n",
      "Rose\n",
      "Ise Love\n",
      "Iy\n",
      "Wynt\n",
      "Qyaina Valencieon\n",
      "Anna James\n",
      "Ry Kidi\n",
      "Ky Devine\n",
      "Sy Devine\n",
      "Nazo\n",
      "Roller\n",
      "Kle\n",
      "Get\n",
      "My Dasl\n",
      "Ve\n",
      "Ye Koli\n",
      "Uchup\n",
      "Nights\n",
      "Beros Bark\n",
      "Rose Penke\n",
      "Cy\n",
      "Sliaz\n",
      "Akness Cox\n",
      "Duckle\n",
      "Xton\n",
      "Ummer\n",
      "Boytes\n",
      "Fertay\n",
      "Yus Phack\n",
      "Jenna Scyde\n",
      "Zi Balkx\n",
      "Aaliaz Ntoll\n",
      "Tóones\n",
      "Lcy\n",
      "Qyuina Love\n",
      "Hones\n",
      "Jfanna Wnights\n",
      "Déithan Lareetee\n",
      "Relae\n",
      "Iga Monroe\n",
      "Iva Lauxen\n",
      "Iranha\n",
      "Lquri\n",
      "Ye Dovine\n",
      "Rucine\n",
      "Pynas Drag Stratton\n",
      "Dia Kelly\n",
      "Nash\n",
      "Quein\n",
      "Oco St James\n",
      "Ve\n",
      "Khes\n",
      "Hue\n",
      "Farkish La Coxx\n",
      "Hole Willing\n",
      "Niou\n",
      "Yurtie\n",
      "Hagoma\n",
      "Fika Lour\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    print(generate_drag_name())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3ad0869ccbffe09a70220b35636623035a236c32668956465565088fb40c07a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
